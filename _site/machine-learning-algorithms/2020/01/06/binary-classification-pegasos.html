<!DOCTYPE html>
<html lang="en"><head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155264062-1"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-155264062-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Binary classification: PEGASOS | Machine Learning with numerical examples</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Binary classification: PEGASOS" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This algorithm is an iterative method, proposed in 2000, to solve a SVM classification without dealing with Quadratic Programming (QP) solutions. Its name comes from Primal Estimated GrAdient SOlver for Svm." />
<meta property="og:description" content="This algorithm is an iterative method, proposed in 2000, to solve a SVM classification without dealing with Quadratic Programming (QP) solutions. Its name comes from Primal Estimated GrAdient SOlver for Svm." />
<link rel="canonical" href="http://localhost:4000/machine-learning-algorithms/2020/01/06/binary-classification-pegasos.html" />
<meta property="og:url" content="http://localhost:4000/machine-learning-algorithms/2020/01/06/binary-classification-pegasos.html" />
<meta property="og:site_name" content="Machine Learning with numerical examples" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-06T00:00:00-04:00" />
<script type="application/ld+json">
{"headline":"Binary classification: PEGASOS","dateModified":"2020-01-06T00:00:00-04:00","datePublished":"2020-01-06T00:00:00-04:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine-learning-algorithms/2020/01/06/binary-classification-pegasos.html"},"url":"http://localhost:4000/machine-learning-algorithms/2020/01/06/binary-classification-pegasos.html","description":"This algorithm is an iterative method, proposed in 2000, to solve a SVM classification without dealing with Quadratic Programming (QP) solutions. Its name comes from Primal Estimated GrAdient SOlver for Svm.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/assets/post.css">
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        messageStyle: 'none',
        tex2jax: {preview: 'none'},
        CommonHTML: {
          scale: 97
        }
      });

      MathJax.Hub.Register.StartupHook("End", function () {
        document.getElementsByClassName('mask_load')[0].style.display='none';
      });
    </script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
    <script src="/assets/jquery_3.3.1.min.js"></script>
    <script src="/assets/post.js"></script><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Machine Learning with numerical examples" /></head>
  <body>
	<div class="container">
	<main>
		<section class="menu_slide menu_invisible" itemscope itemtype="http://schema.org/ItemList">
		
		</section>

		<section class="slides_content">
		<header class="post-header">
			<h1 class="post-title p-name" itemprop="name headline">Binary classification: PEGASOS</h1>
			<p class="post-meta">
				<time class="dt-published" datetime="2020-01-06T00:00:00-04:00" itemprop="datePublished">Jan 6, 2020
				</time></p>

			<i class="fa fa-bars" id="menu"></i>

			<a href="/" title="Home">
				<i class="fa fa-home"></i>
			</a>
			<a href="/about" title="About">
				<i class="fa fa-question-circle"></i>
			</a>
			<a href="/privacy-policies" title="Read privacy policies">
				<i class="fa fa-user-secret"></i>
			</a>
		</header>

		<section class="slides" itemscope itemtype="http://schema.org/PresentationDigitalDocument" itemprop="articleBody">
			<section class="slide">
    <article class="content" itemprop="articleBody">
        <h1>Overview</h1>

        <p class="article">
            This algorithm is an iterative method, proposed in 2000, to solve a SVM classification 
            without dealing with Quadratic Programming (QP) solutions. Its name comes from 
            <span class="highlight">P</span>rimal <span class="highlight">E</span>stimated
            sub-<span class="highlight">G</span>r<span class="highlight">A</span>dient
            <span class="highlight">SO</span>lver for <span class="highlight">S</span>VM. This method 
            works with the primal unconstrained minimization SVM. The main reference is found <a class="link" href="https://www.cs.huji.ac.il/~shais/papers/ShalevSiSrCo10.pdf">here</a>. 
        </p>
    </article>
</section>

<section class="slide">
    <article class="content" itemprop="articleBody">
        <h1>The optimization problem</h1>

        <p class="article">
            The Primal Unconstrained minimization of a SVM is: 
        </p>

        <p class="latex">
            \[min_w: \frac{C}{2}\|\boldsymbol{w}\|^2 + \frac{1}{m}\displaystyle\sum_{i=1}^m max(0, 1-\boldsymbol{y}_i(\boldsymbol{w}^T \boldsymbol{X}_i))\]
        </p>

        <p class="article">
            The objective is to get the values of the vector \(\boldsymbol{w}\). For the classify an unlabeled feature:
        </p>

        <p class="latex">
            \[label=sgn(\boldsymbol{w}^T \boldsymbol{X})\]
        </p>
    </article>
</section>

<section class="slide">
    <article class="content" itemprop="articleBody">
        <h1>The algorithm step by step</h1>

        <ol>
            <li>Set \(\boldsymbol{X}\): the set of features.</li>
            <li>Set \(\boldsymbol{y}\): the set of labels.</li>
            <li>Set \(C\): the penalty term. </li>
            <li>Set \(nI\): the number of instances/samples. </li>
            <li>Set \(nF\): the number of features. </li>
            <li>Set \(B=\{1,2,3,...,nI\}\): a set from 1 to nI elements</li>
            <li>Set \(\boldsymbol{w}=\{0_1,0_2,0_3,...,0_{nF}\}\): a zero-vector with nF elements</li>
            <li>Set \(k\): the number of samples \(\{X,y\}\) to take into consideration at each iteration</li>
            <li>Set \(T=nI\)</li>
            <li>Set \(t=1\)</li>
            <li>If \(t>T\): exit</li>
            <li>Randomly: \(A_t\subseteq B \text{ with }|A_t|=k\), with \(A_t\) of length \(k\)</li>
            <li>
                <p class="latex">
                    \[A_t^+ =\{i\in A_t\mid y_i\langle\boldsymbol{w}_t\boldsymbol{X}_t\rangle < 1\}\]
                </p>
            </li>
            <li>\(s=\frac{1}{Ct}\)</li>
            <li>
                <p class="latex">
                    \[\boldsymbol{w}_{t+1}\leftarrow (1-sC)\boldsymbol{w}_t + \frac{s}{k} \displaystyle\sum_{i\in A_i^+}y_i\boldsymbol{X_i}\]
                </p>
            </li>
            <li>
                <p class="latex">
                    \[\boldsymbol{w}_{t+1}\leftarrow min(1,\frac{1\sqrt{C}}{\|\boldsymbol{w}_{t+1}\|})\]
                </p>
            </li>
            <li>\(k=k+1\)</li>
            <li>Go to step 11</li>
        </ol>
    </article>
</section>

<section class="slide">
    <article class="content" itemprop="articleBody">
        <h1>Python Code</h1>

        <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>

    <span class="n">n_instances</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">n_features</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">global</span> <span class="n">W</span>
    <span class="k">global</span> <span class="n">C</span>
    <span class="k">global</span> <span class="n">b</span>

    <span class="n">W</span><span class="o">=</span><span class="n">w</span>
    <span class="n">b</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">C</span><span class="o">=</span><span class="n">penalty</span>

    <span class="c1"># variables of convergence:
</span>    <span class="n">sum_1</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">sum_2</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">c</span><span class="o">=</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">C</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">A_t_array</span><span class="o">=</span><span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>

        <span class="n">A_t</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_instances</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># random choise subset of n_instances
</span>        <span class="n">A_t_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_t</span><span class="p">)</span>
        <span class="n">A_t_tuned</span><span class="o">=</span><span class="n">getTunedSubset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">A_t</span><span class="p">)</span>

        <span class="n">n_t</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">C</span><span class="o">*</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># step size
</span>        
        <span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="o">*</span><span class="p">(</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">n_t</span><span class="o">*</span><span class="n">C</span> <span class="p">)</span> <span class="o">+</span> <span class="p">(</span> <span class="n">n_t</span> <span class="o">/</span> <span class="n">k</span> <span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">A_t_tuned</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            
        <span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="o">*</span><span class="nb">min</span><span class="p">(</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">C</span><span class="p">)</span> <span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="p">)</span>
        
        <span class="n">sum_1</span><span class="o">+=</span><span class="p">(</span><span class="n">C</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="n">getSumOfHingeLossForA_t</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">A_t</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="n">iterations</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">A_t_array</span><span class="p">:</span>
                <span class="n">sum_2</span><span class="o">+=</span><span class="p">(</span><span class="n">C</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="n">getSumOfHingeLossForA_t</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
                
            <span class="k">if</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">iterations</span><span class="p">)</span><span class="o">*</span><span class="n">sum_1</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">iterations</span><span class="p">)</span><span class="o">*</span><span class="n">sum_2</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">iterations</span><span class="p">)))</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">C</span><span class="o">*</span><span class="n">iterations</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Got optimum w* solution after"</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="s">"iterations"</span><span class="p">)</span>
                
            <span class="k">else</span><span class="p">:</span> 
                <span class="k">print</span><span class="p">(</span><span class="s">"Failed getting optimum w* solution after"</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="s">"iterations"</span><span class="p">)</span>
            
            <span class="n">W</span><span class="o">=</span><span class="n">w</span>
            
            <span class="n">unlabeled</span> <span class="n">sample</span><span class="o">=</span><span class="p">[</span> <span class="mf">0.7534068</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.11394449</span><span class="p">,</span>  <span class="mf">0.71386598</span><span class="p">,</span>  <span class="mf">0.65815634</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.54870557</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23729117</span><span class="p">,</span><span class="o">-</span><span class="mf">0.05749318</span><span class="p">,</span>  <span class="mf">0.43439269</span><span class="p">,</span>  <span class="mf">0.29711878</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0571938</span><span class="p">,</span>   <span class="mf">0.69956576</span><span class="p">,</span>  <span class="mf">0.31596177</span><span class="p">,</span><span class="mf">0.62519434</span><span class="p">,</span>  <span class="mf">0.59411043</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.30600538</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04348982</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.18679726</span><span class="p">,</span>  <span class="mf">0.68839558</span><span class="p">,</span><span class="mf">0.04452071</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11419554</span><span class="p">,</span>  <span class="mf">0.78293974</span><span class="p">,</span>  <span class="mf">0.10141534</span><span class="p">,</span>  <span class="mf">0.69814393</span><span class="p">,</span>  <span class="mf">0.66698174</span><span class="p">,</span><span class="o">-</span><span class="mf">0.6824632</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.26950087</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1937647</span><span class="p">,</span>  <span class="mf">0.49933764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14682283</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6464708</span> <span class="p">]</span>

            <span class="n">type_cancer</span><span class="o">=</span><span class="s">"benign"</span>
            
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">unlabeled</span> <span class="n">sample</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span> <span class="n">type_cancer</span><span class="o">=</span><span class="s">"malignant"</span>
            
            <span class="k">print</span><span class="p">(</span><span class="s">"w="</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
                
            <span class="k">print</span><span class="p">(</span><span class="s">"unlabeled sample = "</span><span class="p">,</span> <span class="n">type_cancer</span><span class="p">)</span>
<span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        </code></pre></figure>
    </article>
</section>

<section class="slide">
    <article class="content" itemprop="articleBody">
        <h1>Example and tests</h1>

        <p class="article">
            Classify a breast cancer in benign or malignant using the breast cancer wisconsin
            (diagnostic) dataset of 1995 provided by the UCI:
        </p>

        <br>

        <p class="article">
            <a class="link" href="/assets/wdbcx.txt" target="_blank">Feature set X</a>
        </p>

        <br>

        <p class="article">
            <a class="link" href="/assets/wdbcy.txt" target="_blank">Label set y</a>
        </p>

        <br>

        <p class="article">
            The above dataset is scaled using <em>numpy</em> and the original can be found <a class="link" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)">here</a>.
        </p>

        <br>

        <p class="article">
            If we run the previous python function, the result will be different in each execution, even if
            the same dataset is used. That is because this algorithm is stochastic, i.e., the output or
            result doesn't depend on the given input:
        </p>

        <br>

        <p class="python_output">
            Got optimum w* solution after 10 iterations 
        </p>
        <p class="python_output">
            w = [-0.15069671, -0.01151086, -0.14325101, -0.14570863, -0.00019509,  -0.02027337, -0.04948397, -0.09071151, -0.02422592,  0.08539943,  -0.11705517, -0.00342588, -0.09828861, -0.13180375,  0.03724529,  -0.02781042, -0.01446634, -0.05062253,  0.00973741,  0.03596281,  -0.18346759, -0.03108999, -0.1677182 , -0.19269945, -0.04153699,  -0.03370983, -0.0383926 , -0.10034278, -0.05628762,  0.04368789]
        </p>
        <p class="python_output">
            unlabeled sample = malignant 
        </p>
        <p class="python_output">
            score: 0.9541284403669725
        </p>

        <br><br>

        <p class="python_output">
            Got optimum w* solution after 10 iterations 
        </p>
        <p class="python_output">
            w = [-0.20686934,  0.1973529 , -0.20770825, -0.21015289, -0.00816597,  -0.09172559, -0.15505612, -0.23583185,  0.10011091,  0.15157233,  -0.1919367 ,  0.17543604, -0.18531842, -0.1728421 , -0.00026198,  -0.04319045, -0.07153597, -0.24532546, -0.04022268,  0.0443598 ,  -0.19122475,  0.16122742, -0.19102686, -0.18262484, -0.06227437,  -0.06407785, -0.09965222, -0.22035173, -0.01040236,  0.00994991]
        </p>
        <p class="python_output">
            unlabeled sample = malignant 
        </p>
        <p class="python_output">
            score: 0.908256880733945
        </p>

        <br><br>

        <p class="python_output">
            Got optimum w* solution after 10 iterations 
        </p>
        <p class="python_output">
            w = [-0.18486092, -0.08499988, -0.17808619, -0.16698979, -0.07370129,  -0.02336567, -0.08975441, -0.09691733,  0.01318181,  0.18290818,  0.03617729, -0.02507894,  0.00354094, -0.01653254, -0.00667443,  0.12991268,  0.08583866,  0.0537965 ,  0.13372307,  0.28272229,  -0.189421  , -0.18694772, -0.17774282, -0.16646996, -0.14889888,  -0.09593508, -0.06173081, -0.13823545, -0.1215791 ,  0.02815563]
        </p>
        <p class="python_output">
            unlabeled sample = malignant 
        </p>
        <p class="python_output">
            score: 0.9174311926605505
        </p>
    </article>
</section>
		</section>

		<section class="buttons_content">
			<button class="btn_action" id="back">
				&lt;
			</button>

			<button class="btn_action" id="next">
				&gt;
			</button>
		</section>
		</section>
	</main>
	</div>
</body>
</html>

